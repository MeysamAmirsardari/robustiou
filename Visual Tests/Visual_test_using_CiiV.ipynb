{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# A test file for CiiV\n",
        "* Ref:\n",
        "  Tang, K., Tao, M., & Zhang, H. (2021). Adversarial visual robustness by causal intervention. arXiv preprint arXiv:2106.09534."
      ],
      "metadata": {
        "id": "frA2BU4MJR-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/MeysamAmirsardari/CiiV-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZINDksFSn4y",
        "outputId": "79855f2f-cc4c-49f0-ced1-3053c507713c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'CiiV-pytorch' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install randaugment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oRK7aQcQHhqI",
        "outputId": "d6678c32-9d2d-4d80-e92e-a9e77e3dce36"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: randaugment in /usr/local/lib/python3.10/dist-packages (1.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports:"
      ],
      "metadata": {
        "id": "06qOVun2JPPL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/CiiV-pytorch/attacker\")\n",
        "sys.path.append(\"/content/CiiV-pytorch/models\")\n",
        "sys.path.append('/content/CiiV-pytorch/data')\n",
        "sys.path.append(\"/content/CiiV-pytorch\")"
      ],
      "metadata": {
        "id": "WSTaJCZSXc8i"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.backends.cudnn as cudnn\n",
        "from torch.autograd import *\n",
        "import torchvision.models as models\n",
        "import importlib\n",
        "import random\n",
        "import attacker\n",
        "\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import yaml\n",
        "import argparse\n",
        "\n",
        "import pickle\n",
        "import json\n",
        "import torch.utils.data as data\n",
        "import torchvision.transforms as transforms\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import sys\n",
        "\n",
        "# from dt_mini_imagenet import miniImageNetData\n",
        "from dt_cifar import CIFAR10Data, CIFAR100Data\n",
        "from dt_toy import TOYData"
      ],
      "metadata": {
        "id": "AnJLrHdtdVhU"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "set the device:"
      ],
      "metadata": {
        "id": "_i9STP9lJnbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "lr = 0.01"
      ],
      "metadata": {
        "id": "U7s_9jF6C6cY"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the dataset:"
      ],
      "metadata": {
        "id": "P-Iou5xYJqPR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # loading Cifar10:\n",
        "\n",
        "# print('==> Preparing data..')\n",
        "# transform_train = transforms.Compose([\n",
        "#     transforms.RandomCrop(32, padding=4),\n",
        "#     transforms.RandomHorizontalFlip(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "# ])\n",
        "\n",
        "# transform_test = transforms.Compose([\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "# ])\n",
        "\n",
        "# trainset = torchvision.datasets.CIFAR10(\n",
        "#     root='./data', train=True, download=True, transform=transform_train)\n",
        "# trainloader = torch.utils.data.DataLoader(\n",
        "#     trainset, batch_size=128, shuffle=True, num_workers=2)\n",
        "\n",
        "# testset = torchvision.datasets.CIFAR10(\n",
        "#     root='./data', train=False, download=True, transform=transform_test)\n",
        "# testloader = torch.utils.data.DataLoader(\n",
        "#     testset, batch_size=100, shuffle=False, num_workers=2)\n",
        "\n",
        "# classes = ('plane', 'car', 'bird', 'cat', 'deer',\n",
        "#            'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "# # Model\n",
        "# print('==> Building model..')\n",
        "# net = torchvision.models.resnet18()\n",
        "# net = net.to(device)\n",
        "# if device == 'cuda':\n",
        "#     net = torch.nn.DataParallel(net)\n",
        "#     cudnn.benchmark = True\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = optim.SGD(net.parameters(), lr=lr,\n",
        "#                       momentum=0.9, weight_decay=5e-4)\n",
        "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n"
      ],
      "metadata": {
        "id": "INPKSER5Cmgs"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rgb_norm(images, config):\n",
        "    mean = [0.4914, 0.4822, 0.4465]\n",
        "    std = [0.2471, 0.2435, 0.2616]\n",
        "    mean = torch.tensor(mean).view(1,3,1,1).to(images.device)\n",
        "    std = torch.tensor(std).view(1,3,1,1).to(images.device)\n",
        "    images = (images - mean) / std\n",
        "    return images"
      ],
      "metadata": {
        "id": "WttmtpbeSZ6V"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the base model:\n",
        "\n",
        "class Base_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Base_Model, self).__init__()\n",
        "        # attacking mode, i.e., generating attacking images\n",
        "        self.attacking = False\n",
        "\n",
        "    def set_attack(self):\n",
        "        self.attacking = True\n",
        "        # recursive set all modules to attack\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Base_Model) and (not m.is_attack()):\n",
        "                m.set_attack()\n",
        "\n",
        "    def set_unattack(self):\n",
        "        self.attacking = False\n",
        "        # recursive set all modules to unattack\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, Base_Model) and m.is_attack():\n",
        "                m.set_unattack()\n",
        "\n",
        "    def is_attack(self):\n",
        "        return self.attacking"
      ],
      "metadata": {
        "id": "T7CF64UtTAsX"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "ffRl2dkzRKY0"
      },
      "outputs": [],
      "source": [
        "#Pre-activation ResNet:\n",
        "\n",
        "class PreActBlock(Base_Model):\n",
        "    # Pre-activation version of the BasicBlock\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, activation='ReLU', softplus_beta=1):\n",
        "        super(PreActBlock, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, track_running_stats=True, affine=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, track_running_stats=True, affine=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "        if activation == 'ReLU':\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            print('ReLU')\n",
        "        elif activation == 'Softplus':\n",
        "            self.relu = nn.Softplus(beta=softplus_beta, threshold=20)\n",
        "            print('Softplus')\n",
        "        elif activation == 'GELU':\n",
        "            self.relu = nn.GELU()\n",
        "            print('GELU')\n",
        "        elif activation == 'ELU':\n",
        "            self.relu = nn.ELU(alpha=1.0, inplace=True)\n",
        "            print('ELU')\n",
        "        elif activation == 'LeakyReLU':\n",
        "            self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "            print('LeakyReLU')\n",
        "        elif activation == 'SELU':\n",
        "            self.relu = nn.SELU(inplace=True)\n",
        "            print('SELU')\n",
        "        elif activation == 'CELU':\n",
        "            self.relu = nn.CELU(alpha=1.2, inplace=True)\n",
        "            print('CELU')\n",
        "        elif activation == 'Tanh':\n",
        "            self.relu = nn.Tanh()\n",
        "            print('Tanh')\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(self.relu(self.bn2(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActBottleneck(Base_Model):\n",
        "    #Pre-activation version of the original Bottleneck module\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, activation='ReLU', softplus_beta=1):\n",
        "        super(PreActBottleneck, self).__init__()\n",
        "        self.bn1 = nn.BatchNorm2d(in_planes, track_running_stats=True, affine=True)\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes, track_running_stats=True, affine=True)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes, track_running_stats=True, affine=True)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False)\n",
        "\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(x))\n",
        "        shortcut = self.shortcut(out) if hasattr(self, 'shortcut') else x\n",
        "        out = self.conv1(out)\n",
        "        out = self.conv2(F.relu(self.bn2(out)))\n",
        "        out = self.conv3(F.relu(self.bn3(out)))\n",
        "        out += shortcut\n",
        "        return out\n",
        "\n",
        "\n",
        "class PreActResNet(Base_Model):\n",
        "    def __init__(self, block, num_blocks, num_classes=10, activation='ReLU', softplus_beta=1,\n",
        "                    num_sample=3, aug_weight=0.9, mask_center=[5, 16, 27]):\n",
        "        super(PreActResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.activation = activation\n",
        "        self.softplus_beta = softplus_beta\n",
        "\n",
        "        self.num_sample = num_sample\n",
        "        self.aug_weight = aug_weight\n",
        "        self.mask_center = mask_center\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.bn = nn.BatchNorm2d(512 * block.expansion, track_running_stats=True, affine=True)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "\n",
        "        if activation == 'ReLU':\n",
        "            self.relu = nn.ReLU(inplace=True)\n",
        "            print('ReLU')\n",
        "        elif activation == 'Softplus':\n",
        "            self.relu = nn.Softplus(beta=softplus_beta, threshold=20)\n",
        "            print('Softplus')\n",
        "        elif activation == 'GELU':\n",
        "            self.relu = nn.GELU()\n",
        "            print('GELU')\n",
        "        elif activation == 'ELU':\n",
        "            self.relu = nn.ELU(alpha=1.0, inplace=True)\n",
        "            print('ELU')\n",
        "        elif activation == 'LeakyReLU':\n",
        "            self.relu = nn.LeakyReLU(negative_slope=0.1, inplace=True)\n",
        "            print('LeakyReLU')\n",
        "        elif activation == 'SELU':\n",
        "            self.relu = nn.SELU(inplace=True)\n",
        "            print('SELU')\n",
        "        elif activation == 'CELU':\n",
        "            self.relu = nn.CELU(alpha=1.2, inplace=True)\n",
        "            print('CELU')\n",
        "        elif activation == 'Tanh':\n",
        "            self.relu = nn.Tanh()\n",
        "            print('Tanh')\n",
        "        print('Use activation of ' + activation)\n",
        "\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride,\n",
        "                activation=self.activation, softplus_beta=self.softplus_beta))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def create_mask(self, w, h, center_x, center_y, alpha=10.0):\n",
        "        widths = torch.arange(w).view(1, -1).repeat(h,1)\n",
        "        heights = torch.arange(h).view(-1, 1).repeat(1,w)\n",
        "        mask = ((widths - center_x)**2 + (heights - center_y)**2).float().sqrt()\n",
        "        # non-linear\n",
        "        mask = (mask.max() - mask + alpha) ** 0.3\n",
        "        mask = mask / mask.max()\n",
        "        # sampling\n",
        "        mask = (mask + mask.clone().uniform_(0, 1)) > 0.9\n",
        "        mask.float()\n",
        "        return mask.unsqueeze(0)\n",
        "\n",
        "    def create_mask_candidate1(self, w, h, center_x, center_y, alpha=10.0):\n",
        "        widths = torch.arange(w).view(1, -1).repeat(h,1)\n",
        "        heights = torch.arange(h).view(-1, 1).repeat(1,w)\n",
        "        mask = ((widths - center_x)**2 + (heights - center_y)**2).float().sqrt()\n",
        "        # non-linear\n",
        "        mask = 1.0 - mask / 120\n",
        "        # sampling\n",
        "        mask = (mask + mask.clone().uniform_(0, 1)) > 0.9\n",
        "        mask.float()\n",
        "        return mask.unsqueeze(0)\n",
        "\n",
        "    def create_mask_candidate2(self, w, h, center_x, center_y, alpha=10.0):\n",
        "        widths = torch.arange(w).view(1, -1).repeat(h,1)\n",
        "        heights = torch.arange(h).view(-1, 1).repeat(1,w)\n",
        "        mask = ((widths - center_x)**2 + (heights - center_y)**2).float().sqrt()\n",
        "        # non-linear\n",
        "        mask = 2.5 / (0.6 * mask**0.5)\n",
        "        # sampling\n",
        "        mask = (mask + mask.clone().uniform_(0, 1)) > 0.9\n",
        "        mask.float()\n",
        "        return mask.unsqueeze(0)\n",
        "\n",
        "    def ciiv_forward(self, x, loop):\n",
        "        b, c, w, h = x.shape\n",
        "        samples = []\n",
        "        masks = []\n",
        "        NUM_LOOP = loop\n",
        "        NUM_INNER_SAMPLE = self.num_sample\n",
        "        NUM_TOTAL_SAMPLE = NUM_LOOP * NUM_INNER_SAMPLE\n",
        "\n",
        "        # generate all samples\n",
        "        for i in range(NUM_TOTAL_SAMPLE):\n",
        "            # differentiable sampling\n",
        "            sample = self.relu(x + x.detach().clone().uniform_(-1,1) * self.aug_weight)\n",
        "            sample = sample / (sample + 1e-5)\n",
        "            #on_sample = torch.clamp(x + torch.randn_like(x) * 0.1, min=0, max=1)\n",
        "            if i % NUM_INNER_SAMPLE == 0:\n",
        "                idx = int(i // NUM_INNER_SAMPLE)\n",
        "                x_idx = int(idx // 3)\n",
        "                y_idx = int(idx % 3)\n",
        "                center_x = self.mask_center[x_idx]\n",
        "                center_y = self.mask_center[y_idx]\n",
        "            # attention\n",
        "            mask = self.create_mask(w, h, center_x, center_y, alpha=10.0).to(x.device)\n",
        "            sample = sample * mask\n",
        "            samples.append(sample)\n",
        "            masks.append(mask)\n",
        "\n",
        "        # run network\n",
        "        outputs = []\n",
        "        features = []\n",
        "        z_scores = []\n",
        "        for i in range(NUM_LOOP):\n",
        "            # Normalized input\n",
        "            inputs = sum(samples[NUM_INNER_SAMPLE * i : NUM_INNER_SAMPLE * (i+1)]) / NUM_INNER_SAMPLE\n",
        "            z_score = (sum(masks[NUM_INNER_SAMPLE * i : NUM_INNER_SAMPLE * (i+1)]).float() / NUM_INNER_SAMPLE).mean()\n",
        "            # forward modules\n",
        "            out = self.conv1(inputs)\n",
        "            out = self.layer1(out)\n",
        "            out = self.layer2(out)\n",
        "            out = self.layer3(out)\n",
        "            out = self.layer4(out)\n",
        "            out = self.relu(self.bn(out))\n",
        "            size = out.shape[-1]\n",
        "            out = F.avg_pool2d(out, size)\n",
        "            feats = out.view(out.size(0), -1)\n",
        "            preds = self.linear(feats)\n",
        "            z_scores.append(z_score.view(1,1).repeat(b, 1))\n",
        "            features.append(feats)\n",
        "            outputs.append(preds)\n",
        "\n",
        "        final_pred = sum([pred / (z + 1e-9) for pred, z in zip(outputs, z_scores)]) / NUM_LOOP\n",
        "\n",
        "        ## Randomized Smoothing Inference\n",
        "        #if self.training or self.is_attack():\n",
        "        #    final_pred = sum([pred / (z + 1e-9) for pred, z in zip(outputs, z_scores)]) / NUM_LOOP\n",
        "        #else:\n",
        "        #    counts = []\n",
        "        #    for item in outputs:\n",
        "        #        pred = item.max(-1)[1]\n",
        "        #        counts.append(F.one_hot(pred, self.num_classes))\n",
        "        #    final_pred = sum(counts)\n",
        "        return final_pred, z_scores, features, outputs\n",
        "\n",
        "\n",
        "    def forward(self, x, loop=1):\n",
        "        if x.shape[1] == 1:\n",
        "            x = x.repeat(1, 3, 1, 1)\n",
        "        # config is passed through main\n",
        "        x = rgb_norm(x, self.config)\n",
        "\n",
        "        if self.training:\n",
        "            return self.ciiv_forward(x, loop=loop)\n",
        "        else:\n",
        "            return self.ciiv_forward(x, loop=loop)[0]\n",
        "\n",
        "\n",
        "def PreActResNet18(num_classes=10, activation='ReLU', softplus_beta=1, **kwargs):\n",
        "    return PreActResNet(PreActBlock, [2,2,2,2], num_classes=num_classes, activation=activation, softplus_beta=softplus_beta, **kwargs)\n",
        "\n",
        "def PreActResNet34(num_classes, **kwargs):\n",
        "    return PreActResNet(PreActBlock, [3,4,6,3], num_classes, **kwargs)\n",
        "\n",
        "def PreActResNet50(num_classes, **kwargs):\n",
        "    return PreActResNet(PreActBottleneck, [3,4,6,3], num_classes, **kwargs)\n",
        "\n",
        "\n",
        "def create_model(m_type='resnet18', num_classes=1000, num_sample=3, aug_weight=0.9, mask_center=[5, 16, 27]):\n",
        "    # create various resnet models\n",
        "    if m_type == 'resnet18':\n",
        "        model = PreActResNet18(num_classes=num_classes, num_sample=num_sample,\n",
        "                                aug_weight=aug_weight, mask_center=mask_center)\n",
        "    elif m_type == 'resnet50':\n",
        "        model = PreActResNet50(num_classes=num_classes, num_sample=num_sample,\n",
        "                                aug_weight=aug_weight, mask_center=mask_center)\n",
        "    else:\n",
        "        raise ValueError('Wrong Model Type')\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_adv_init(config):\n",
        "    if random.uniform(0, 1) < config['attacker_opt']['attack_rand_ini']:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def create_adversarial_attacker(config, model, logger):\n",
        "    if config['attacker_opt']['attack_type'] == 'PGD':\n",
        "        return attacker.PGD(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'PGDL2':\n",
        "        return attacker.PGDL2(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'AutoAttack':\n",
        "        return attacker.AA(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                norm=config['attacker_opt']['attack_norm'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'FGSM':\n",
        "        return attacker.FGSM(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'FFGSM':\n",
        "        return attacker.FFGSM(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'GN':\n",
        "        return attacker.GN(model, logger, config, sigma=config['attacker_opt']['gn_sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'UN':\n",
        "        return attacker.UN(model, logger, config, sigma=config['attacker_opt']['un_sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'BPDAPGD':\n",
        "        return attacker.BPDAPGD(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'EOT':\n",
        "        return attacker.EOT(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                learning_rate=config['attacker_opt']['attack_lr'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'CW':\n",
        "        return attacker.CW(model, logger, config, c=config['attacker_opt']['c'],\n",
        "                                                kappa=config['attacker_opt']['kappa'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                                lr=config['attacker_opt']['lr'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'BFS':\n",
        "        return attacker.BFS(model, logger, config, sigma=config['attacker_opt']['sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'SPSA':\n",
        "        return attacker.SPSA(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                delta=config['attacker_opt']['delta'],\n",
        "                                                batch_size=config['attacker_opt']['batch_size'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                                lr=config['attacker_opt']['lr'],\n",
        "                                            )\n",
        "    else:\n",
        "        logger.raise_error('Wrong Attacker Type')\n",
        "\n",
        "\n",
        "\n",
        "def get_adv_target(target_type, model, inputs, gt_label):\n",
        "    with torch.no_grad():\n",
        "        preds = model(inputs).softmax(-1)\n",
        "    num_batch, num_class = preds.shape\n",
        "\n",
        "    if target_type == 'random':\n",
        "        adv_targets = torch.randint(0, num_class, (num_batch,)).to(gt_label.device)\n",
        "        # validation check\n",
        "        adv_targets = adv_target_update(gt_label, adv_targets, num_batch, num_class)\n",
        "    elif target_type == 'most':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = -1\n",
        "        adv_targets = preds.max(-1)[1]\n",
        "    elif target_type == 'least':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = 100.0\n",
        "        adv_targets = preds.min(-1)[1]\n",
        "    else:\n",
        "        raise ValueError('Wrong Targeted Attack Type')\n",
        "\n",
        "    assert (adv_targets == gt_label).long().sum().item() == 0\n",
        "    return adv_targets\n",
        "\n",
        "def adv_target_update(gt_label, adv_target, num_batch, num_class):\n",
        "    for i in range(num_batch):\n",
        "        if int(gt_label[i]) == int(adv_target[i]):\n",
        "            adv_target[i] = (int(adv_target[i]) + random.randint(1, num_class-1)) % num_class\n",
        "    return adv_target"
      ],
      "metadata": {
        "id": "RX8xd-_xUhej"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def rand_adv_init(config):\n",
        "    if random.uniform(0, 1) < config['attacker_opt']['attack_rand_ini']:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def create_adversarial_attacker(config, model, logger):\n",
        "    if config['attacker_opt']['attack_type'] == 'PGD':\n",
        "        return attacker.PGD(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'PGDL2':\n",
        "        return attacker.PGDL2(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'AutoAttack':\n",
        "        return attacker.AA(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                norm=config['attacker_opt']['attack_norm'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'FGSM':\n",
        "        return attacker.FGSM(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'FFGSM':\n",
        "        return attacker.FFGSM(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'GN':\n",
        "        return attacker.GN(model, logger, config, sigma=config['attacker_opt']['gn_sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'UN':\n",
        "        return attacker.UN(model, logger, config, sigma=config['attacker_opt']['un_sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'BPDAPGD':\n",
        "        return attacker.BPDAPGD(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'EOT':\n",
        "        return attacker.EOT(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                learning_rate=config['attacker_opt']['attack_lr'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'CW':\n",
        "        return attacker.CW(model, logger, config, c=config['attacker_opt']['c'],\n",
        "                                                kappa=config['attacker_opt']['kappa'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                                lr=config['attacker_opt']['lr'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'BFS':\n",
        "        return attacker.BFS(model, logger, config, sigma=config['attacker_opt']['sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'SPSA':\n",
        "        return attacker.SPSA(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                delta=config['attacker_opt']['delta'],\n",
        "                                                batch_size=config['attacker_opt']['batch_size'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                                lr=config['attacker_opt']['lr'],\n",
        "                                            )\n",
        "    else:\n",
        "        logger.raise_error('Wrong Attacker Type')\n",
        "\n",
        "\n",
        "\n",
        "def get_adv_target(target_type, model, inputs, gt_label):\n",
        "    with torch.no_grad():\n",
        "        preds = model(inputs).softmax(-1)\n",
        "    num_batch, num_class = preds.shape\n",
        "\n",
        "    if target_type == 'random':\n",
        "        adv_targets = torch.randint(0, num_class, (num_batch,)).to(gt_label.device)\n",
        "        # validation check\n",
        "        adv_targets = adv_target_update(gt_label, adv_targets, num_batch, num_class)\n",
        "    elif target_type == 'most':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = -1\n",
        "        adv_targets = preds.max(-1)[1]\n",
        "    elif target_type == 'least':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = 100.0\n",
        "        adv_targets = preds.min(-1)[1]\n",
        "    else:\n",
        "        raise ValueError('Wrong Targeted Attack Type')\n",
        "\n",
        "    assert (adv_targets == gt_label).long().sum().item() == 0\n",
        "    return adv_targets\n",
        "\n",
        "def adv_target_update(gt_label, adv_target, num_batch, num_class):\n",
        "    for i in range(num_batch):\n",
        "        if int(gt_label[i]) == int(adv_target[i]):\n",
        "            adv_target[i] = (int(adv_target[i]) + random.randint(1, num_class-1)) % num_class\n",
        "    return adv_target"
      ],
      "metadata": {
        "id": "PSXBo2MOq_Qr"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "\n",
        "class test_ciiv():\n",
        "    def __init__(self, config, logger, model, val=False):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.model = model\n",
        "\n",
        "        # init inst setting\n",
        "        self.init_inst_sample()\n",
        "\n",
        "        # initialize attacker\n",
        "        self.adv_test = self.config['attacker_opt']['adv_val']\n",
        "        if self.adv_test:\n",
        "            self.attacker = create_adversarial_attacker(config, model, logger)\n",
        "\n",
        "        # save test\n",
        "        self.test_save = True if config['test_opt']['save_data'] else False\n",
        "\n",
        "        # get dataloader\n",
        "        if val:\n",
        "            self.phase = 'val'\n",
        "            self.loader = get_loader(config, 'val', logger)\n",
        "        else:\n",
        "            self.phase = 'test'\n",
        "            self.loader = get_loader(config, 'val', logger)\n",
        "\n",
        "    def init_inst_sample(self):\n",
        "        self.logger.info('=====> Init Instrumental Sampling')\n",
        "        self.w_ce = self.config['inst_sample']['w_ce']\n",
        "        self.w_reg = self.config['inst_sample']['w_reg']\n",
        "        self.mul_ru = self.config['inst_sample']['mul_ru']\n",
        "        self.num_loop = self.config['inst_sample']['num_loop']\n",
        "\n",
        "\n",
        "    def run_val(self, epoch):\n",
        "        self.logger.info('------------- Start Validation at Epoch {} -----------'.format(epoch))\n",
        "        total_acc = []\n",
        "\n",
        "        # set model to evaluation\n",
        "        self.model.eval()\n",
        "\n",
        "        # save test\n",
        "        if self.test_save:\n",
        "            org_list = []\n",
        "            adv_list = []\n",
        "            gt_list = []\n",
        "            pred_list = []\n",
        "\n",
        "        # run batch\n",
        "        for i, (inputs, labels, indexes) in enumerate(self.loader):\n",
        "            inputs, labels = inputs.cuda(), labels.cuda()\n",
        "            batch_size = inputs.shape[0]\n",
        "            # print test time\n",
        "            # trigger adversarial attack or not\n",
        "            if self.adv_test:\n",
        "                if self.config['targeted_attack']:\n",
        "                    adv_targets = get_adv_target(self.config['targeted_type'], self.model, inputs, labels)\n",
        "                    adv_inputs = self.attacker.get_adv_images(inputs, adv_targets, random_start=rand_adv_init(self.config), targeted=True)\n",
        "                else:\n",
        "                    adv_inputs = self.attacker.get_adv_images(inputs, labels, random_start=rand_adv_init(self.config))\n",
        "                self.model.eval()\n",
        "                final_inputs = adv_inputs\n",
        "            else:\n",
        "                final_inputs = inputs\n",
        "\n",
        "            # run model\n",
        "            with torch.no_grad():\n",
        "                predictions = self.model(final_inputs, loop=self.num_loop)\n",
        "\n",
        "            if isinstance(predictions, tuple):\n",
        "                predictions = predictions[0]\n",
        "\n",
        "            total_acc.append((predictions.max(1)[1] == labels).view(-1, 1))\n",
        "\n",
        "            # save adversarial images\n",
        "            if self.test_save and i < self.config['test_opt']['save_length']:\n",
        "                org_list.append(inputs.cpu())\n",
        "                gt_list.append(labels.cpu())\n",
        "                pred_list.append(predictions.max(1)[1].cpu())\n",
        "                if self.adv_test:\n",
        "                    adv_list.append(adv_inputs.cpu())\n",
        "\n",
        "        all_acc = torch.cat(total_acc, dim=0).float()\n",
        "        avg_acc = all_acc.mean().item()\n",
        "        self.logger.info('Epoch {:5d} Evaluation Complete ==> Total Accuracy : {:9.4f}, Number Samples : {:9d}'.format(epoch, avg_acc, all_acc.shape[0]))\n",
        "\n",
        "        # set back to training mode again\n",
        "        self.model.train()\n",
        "\n",
        "        # save adversarial images\n",
        "        if self.test_save:\n",
        "            file_name = os.path.join(self.config['output_dir'], self.config['test_opt']['file_name'])\n",
        "            adv_output = {\n",
        "                    'org_images' : torch.cat(org_list, 0),\n",
        "                    'gt_labels'  : torch.cat(gt_list, 0),\n",
        "                    'adv_images' : torch.cat(adv_list, 0) if self.adv_test else 0,\n",
        "                    'pred_labels': torch.cat(pred_list, 0),\n",
        "                    }\n",
        "            torch.save(adv_output, file_name)\n",
        "            self.logger.info('=====> Complete! Adversarial images have been saved to {}'.format(file_name))\n",
        "\n",
        "        return avg_acc"
      ],
      "metadata": {
        "id": "9ZPPODfZZv5f"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rand_adv_init(config):\n",
        "    if random.uniform(0, 1) < config['attacker_opt']['attack_rand_ini']:\n",
        "        return True\n",
        "    else:\n",
        "        return False\n",
        "\n",
        "\n",
        "def create_adversarial_attacker(config, model, logger):\n",
        "    if config['attacker_opt']['attack_type'] == 'PGD':\n",
        "        return attacker.PGD(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'PGDL2':\n",
        "        return attacker.PGDL2(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'AutoAttack':\n",
        "        return attacker.AA(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                norm=config['attacker_opt']['attack_norm'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'FGSM':\n",
        "        return attacker.FGSM(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'FFGSM':\n",
        "        return attacker.FFGSM(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'GN':\n",
        "        return attacker.GN(model, logger, config, sigma=config['attacker_opt']['gn_sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'UN':\n",
        "        return attacker.UN(model, logger, config, sigma=config['attacker_opt']['un_sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'BPDAPGD':\n",
        "        return attacker.BPDAPGD(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                alpha=config['attacker_opt']['attack_alpha'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'EOT':\n",
        "        return attacker.EOT(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                learning_rate=config['attacker_opt']['attack_lr'],\n",
        "                                                steps=config['attacker_opt']['attack_step'],\n",
        "                                                eot_iter=config['attacker_opt']['eot_iter'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'CW':\n",
        "        return attacker.CW(model, logger, config, c=config['attacker_opt']['c'],\n",
        "                                                kappa=config['attacker_opt']['kappa'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                                lr=config['attacker_opt']['lr'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'BFS':\n",
        "        return attacker.BFS(model, logger, config, sigma=config['attacker_opt']['sigma'],\n",
        "                                                eps=config['attacker_opt']['attack_eps'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                            )\n",
        "    elif config['attacker_opt']['attack_type'] == 'SPSA':\n",
        "        return attacker.SPSA(model, logger, config, eps=config['attacker_opt']['attack_eps'],\n",
        "                                                delta=config['attacker_opt']['delta'],\n",
        "                                                batch_size=config['attacker_opt']['batch_size'],\n",
        "                                                steps=config['attacker_opt']['steps'],\n",
        "                                                lr=config['attacker_opt']['lr'],\n",
        "                                            )\n",
        "    else:\n",
        "        logger.raise_error('Wrong Attacker Type')\n",
        "\n",
        "\n",
        "\n",
        "def get_adv_target(target_type, model, inputs, gt_label):\n",
        "    with torch.no_grad():\n",
        "        preds = model(inputs).softmax(-1)\n",
        "    num_batch, num_class = preds.shape\n",
        "\n",
        "    if target_type == 'random':\n",
        "        adv_targets = torch.randint(0, num_class, (num_batch,)).to(gt_label.device)\n",
        "        # validation check\n",
        "        adv_targets = adv_target_update(gt_label, adv_targets, num_batch, num_class)\n",
        "    elif target_type == 'most':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = -1\n",
        "        adv_targets = preds.max(-1)[1]\n",
        "    elif target_type == 'least':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = 100.0\n",
        "        adv_targets = preds.min(-1)[1]\n",
        "    else:\n",
        "        raise ValueError('Wrong Targeted Attack Type')\n",
        "\n",
        "    assert (adv_targets == gt_label).long().sum().item() == 0\n",
        "    return adv_targets\n",
        "\n",
        "def adv_target_update(gt_label, adv_target, num_batch, num_class):\n",
        "    for i in range(num_batch):\n",
        "        if int(gt_label[i]) == int(adv_target[i]):\n",
        "            adv_target[i] = (int(adv_target[i]) + random.randint(1, num_class-1)) % num_class\n",
        "    return adv_target"
      ],
      "metadata": {
        "id": "g6Qanz6VrOTj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_test_func(config):\n",
        "    # choosing test strategy\n",
        "    if config['strategy']['test_type'] == 'baseline':\n",
        "        test_func = test_baseline\n",
        "    elif config['strategy']['test_type'] == 'ciiv':\n",
        "        test_func = test_ciiv\n",
        "    else:\n",
        "        raise ValueError('Wrong Test Strategy')\n",
        "    return test_func\n",
        "\n",
        "\n",
        "\n",
        "def get_adv_target(target_type, model, inputs, gt_label):\n",
        "    with torch.no_grad():\n",
        "        preds = model(inputs).softmax(-1)\n",
        "    num_batch, num_class = preds.shape\n",
        "\n",
        "    if target_type == 'random':\n",
        "        adv_targets = torch.randint(0, num_class, (num_batch,)).to(gt_label.device)\n",
        "        # validation check\n",
        "        adv_targets = adv_target_update(gt_label, adv_targets, num_batch, num_class)\n",
        "    elif target_type == 'most':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = -1\n",
        "        adv_targets = preds.max(-1)[1]\n",
        "    elif target_type == 'least':\n",
        "        idxs = torch.arange(num_batch).to(inputs.device)\n",
        "        preds[idxs, gt_label] = 100.0\n",
        "        adv_targets = preds.min(-1)[1]\n",
        "    else:\n",
        "        raise ValueError('Wrong Targeted Attack Type')\n",
        "\n",
        "    assert (adv_targets == gt_label).long().sum().item() == 0\n",
        "    return adv_targets\n",
        "\n",
        "def adv_target_update(gt_label, adv_target, num_batch, num_class):\n",
        "    for i in range(num_batch):\n",
        "        if int(gt_label[i]) == int(adv_target[i]):\n",
        "            adv_target[i] = (int(adv_target[i]) + random.randint(0, num_class-1)) % num_class\n",
        "    return adv_target"
      ],
      "metadata": {
        "id": "zwzudcu_Ut0S"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for checkpoints:\n",
        "\n",
        "class Checkpoint():\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.save_best = config['checkpoint_opt']['save_best']\n",
        "        self.best_epoch = -1\n",
        "        self.best_performance = -1\n",
        "        self.best_model_path = None\n",
        "\n",
        "\n",
        "    def save(self, model, epoch, logger, acc):\n",
        "        # update best model\n",
        "        model_name = 'epoch_{}_'.format(epoch) + self.config['checkpoint_opt']['checkpoint_name']\n",
        "        model_path = os.path.join(self.config['output_dir'], model_name)\n",
        "        if acc is not None:\n",
        "            if float(acc) > self.best_performance:\n",
        "                self.best_epoch = epoch\n",
        "                self.best_performance = float(acc)\n",
        "                self.best_model_path = model_path\n",
        "                self.save_current = True\n",
        "                logger.info('Best model is updated at epoch {} with accuracy {:9.3f} (Path: {})'.format(self.best_epoch, self.best_performance, self.best_model_path))\n",
        "            else:\n",
        "                self.save_current = False\n",
        "        else:\n",
        "            # if acc is None, the newest is always the best\n",
        "            self.best_epoch = epoch\n",
        "            self.best_model_path = model_path\n",
        "\n",
        "        # only save the best model or the last model\n",
        "        best_saving = (self.save_best and self.save_current)\n",
        "        # only save at certain steps, best epoch or the last epoch\n",
        "        if (not best_saving) and (epoch % self.config['checkpoint_opt']['checkpoint_step'] != 0) and (epoch < (self.config['training_opt']['num_epochs'] - 1)):\n",
        "            return\n",
        "\n",
        "        output = {\n",
        "            'state_dict': model.state_dict(),\n",
        "            'epoch': epoch,\n",
        "        }\n",
        "\n",
        "        logger.info('Model at epoch {} is saved to {}'.format(epoch, model_path))\n",
        "        torch.save(output, model_path)\n",
        "        logger.info('Best model is at epoch {} with accuracy {:9.3f}'.format(self.best_epoch, self.best_performance))\n",
        "        self.save_best_model(logger)\n",
        "\n",
        "\n",
        "    def save_best_model(self, logger):\n",
        "        logger.info('Best model is at epoch {} with accuracy {:9.3f} (Path: {})'.format(self.best_epoch, self.best_performance, self.best_model_path))\n",
        "        with open(os.path.join(self.config['output_dir'], 'best_checkpoint'), 'w+') as f:\n",
        "            f.write(self.best_model_path + ' ' + str(self.best_epoch) + ' ' + str(self.best_performance) + '\\n')\n",
        "\n",
        "    def load(self, model, path, logger):\n",
        "        if path.split('.')[-1] != 'pth':\n",
        "            with open(os.path.join(path, 'best_checkpoint')) as f:\n",
        "                path = f[0].split(' ')[0]\n",
        "\n",
        "        checkpoint = torch.load(path, map_location='cpu')\n",
        "        logger.info('Loading checkpoint pretrained with epoch {}.'.format(checkpoint['epoch']))\n",
        "        model_state = checkpoint['state_dict']\n",
        "\n",
        "        x = model.state_dict()\n",
        "        for key, _ in x.items():\n",
        "            if key in model_state:\n",
        "                x[key] = model_state[key]\n",
        "                logger.info('Load {:>50} from checkpoint.'.format(key))\n",
        "            elif 'module.' + key in model_state:\n",
        "                x[key] = model_state['module.' + key]\n",
        "                logger.info('Load {:>50} from checkpoint (rematch with module.).'.format(key))\n",
        "            else:\n",
        "                logger.info('WARNING: Key {} is missing in the checkpoint.'.format(key))\n",
        "\n",
        "        model.load_state_dict(x)\n",
        "        pass\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JbJxb0KUrpAb"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# return a dataloader\n",
        "\n",
        "def get_loader(config, phase, logger):\n",
        "    dataset = config['dataset']['name']\n",
        "    if dataset == 'mini-imagenet':\n",
        "        split = miniImageNetData(phase=phase,\n",
        "                             data_path=config['dataset']['data_path'],\n",
        "                             category_path=config['dataset']['category_path'],\n",
        "                             train_path=config['dataset']['train_path'],\n",
        "                             test_path=config['dataset']['test_path'],\n",
        "                             val_path=config['dataset']['val_path'],\n",
        "                             output_path=config['output_dir'],\n",
        "                             num_classes=config['dataset']['num_classes'],\n",
        "                             rand_aug=config['dataset']['rand_aug'],\n",
        "                             logger=logger)\n",
        "    elif dataset == 'cifar10':\n",
        "        split = CIFAR10Data(phase=phase,\n",
        "                             data_path=config['dataset']['data_path'],\n",
        "                             output_path=config['output_dir'],\n",
        "                             rand_aug=config['dataset']['rand_aug'],\n",
        "                             logger=logger)\n",
        "    elif dataset == 'cifar100':\n",
        "        split = CIFAR100Data(phase=phase,\n",
        "                             data_path=config['dataset']['data_path'],\n",
        "                             output_path=config['output_dir'],\n",
        "                             rand_aug=config['dataset']['rand_aug'],\n",
        "                             logger=logger)\n",
        "    elif dataset == 'TOYData':\n",
        "        split = TOYData(phase=phase,\n",
        "                        output_path=config['output_dir'],\n",
        "                        num_classes=config['dataset']['num_classes'],\n",
        "                        logger=logger)\n",
        "    else:\n",
        "        logger.info('********** ERROR: unidentified dataset **********')\n",
        "\n",
        "    loader = data.DataLoader(\n",
        "        split,\n",
        "        batch_size=config['training_opt']['batch_size'],\n",
        "        shuffle=True if phase == 'train' else False,\n",
        "        pin_memory=True,\n",
        "        num_workers=config['training_opt']['data_workers'],\n",
        "    )\n",
        "\n",
        "    return loader\n"
      ],
      "metadata": {
        "id": "-vgveRcpF1HH"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.optim as optim\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import math\n",
        "\n",
        "\n",
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes=10):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.cls = classes\n",
        "\n",
        "    def forward(self, pred, target, confidence, dim=-1):\n",
        "        smoothing = 1.0 - confidence\n",
        "        pred = pred.log_softmax(dim=dim)\n",
        "        with torch.no_grad():\n",
        "            # true_dist = pred.data.clone()\n",
        "            true_dist = torch.zeros_like(pred)\n",
        "            true_dist.fill_(smoothing / (self.cls - 1))\n",
        "            true_dist.scatter_(1, target.data.unsqueeze(1), confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * pred, dim=dim))\n",
        "\n",
        "class train_ciiv():\n",
        "    def __init__(self, config, logger, model, eval=False):\n",
        "        self.config = config\n",
        "        self.logger = logger\n",
        "        self.model = model\n",
        "        self.training_opt = config['training_opt']\n",
        "        self.checkpoint = Checkpoint(config)\n",
        "        self.logger.info('============= Training Strategy: CiiV Training ============')\n",
        "\n",
        "        # get dataloader\n",
        "        self.logger.info('=====> Get train dataloader')\n",
        "        self.train_loader = get_loader(config, 'train', logger)\n",
        "\n",
        "        # init inst setting\n",
        "        self.init_inst_sample()\n",
        "\n",
        "        # create optimizer\n",
        "        self.create_optimizer()\n",
        "\n",
        "        # create scheduler\n",
        "        self.create_scheduler()\n",
        "\n",
        "        # create loss\n",
        "        self.creat_loss()\n",
        "\n",
        "        # adversarial train\n",
        "        self.adv_train = self.config['attacker_opt']['adv_train']\n",
        "        if self.adv_train:\n",
        "            self.attacker = create_adversarial_attacker(config, model, logger)\n",
        "\n",
        "        # set eval\n",
        "        # if eval:\n",
        "        # choosing test strategy\n",
        "        test_func = get_test_func(config)\n",
        "        # start testing\n",
        "        self.testing = test_func(config, logger, model, val=True)\n",
        "\n",
        "    def init_inst_sample(self):\n",
        "        self.logger.info('=====> Init Instrumental Sampling')\n",
        "        self.w_ce = self.config['inst_sample']['w_ce']\n",
        "        self.w_reg = self.config['inst_sample']['w_reg']\n",
        "        self.mul_ru = self.config['inst_sample']['mul_ru']\n",
        "        self.num_loop = self.config['inst_sample']['num_loop']\n",
        "\n",
        "    def creat_loss(self):\n",
        "        if self.config['inst_sample']['ce_smooth']:\n",
        "            self.criterion = LabelSmoothingLoss(classes=self.config['networks']['params']['num_classes'])\n",
        "        else:\n",
        "            self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def create_optimizer(self):\n",
        "        self.logger.info('=====> Create optimizer')\n",
        "        optim_params = self.training_opt['optim_params']\n",
        "        optim_params_dict = {'params': self.model.parameters(),\n",
        "                            'lr': optim_params['lr'],\n",
        "                            'momentum': optim_params['momentum'],\n",
        "                            'weight_decay': optim_params['weight_decay']\n",
        "                            }\n",
        "\n",
        "        if self.training_opt['optimizer'] == 'Adam':\n",
        "            self.optimizer = optim.Adam([optim_params_dict, ])\n",
        "        elif self.training_opt['optimizer'] == 'SGD':\n",
        "            self.optimizer = optim.SGD([optim_params_dict, ])\n",
        "        else:\n",
        "            self.logger.info('********** ERROR: unidentified optimizer **********')\n",
        "\n",
        "\n",
        "    def create_scheduler(self):\n",
        "        self.logger.info('=====> Create Scheduler')\n",
        "        scheduler_params = self.training_opt['scheduler_params']\n",
        "        if self.training_opt['scheduler'] == 'cosine':\n",
        "            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(self.optimizer, self.training_opt['num_epochs'], eta_min=scheduler_params['endlr'])\n",
        "        elif self.training_opt['scheduler'] == 'step':\n",
        "            self.scheduler = optim.lr_scheduler.MultiStepLR(self.optimizer, gamma=scheduler_params['gamma'], milestones=scheduler_params['milestones'])\n",
        "        else:\n",
        "            self.logger.info('********** ERROR: unidentified optimizer **********')\n",
        "\n",
        "    def l2_loss(self, x, y):\n",
        "        diff = x - y\n",
        "        diff = diff*diff\n",
        "        diff = diff.sum(1)\n",
        "        diff = diff.mean(0)\n",
        "        return diff\n",
        "\n",
        "    def smooth_l1_loss(self, x, y):\n",
        "        diff = F.smooth_l1_loss(x, y, reduction='none')\n",
        "        diff = diff.sum(1)\n",
        "        diff = diff.mean(0)\n",
        "        return diff\n",
        "\n",
        "    def get_mean_wo_i(self, inputs, i):\n",
        "        return (sum(inputs) - inputs[i]) / float(len(inputs) - 1)\n",
        "\n",
        "    def run(self):\n",
        "        # Start Training\n",
        "        self.logger.info('=====> Start Naive Training')\n",
        "\n",
        "        # run epoch\n",
        "        for epoch in range(self.training_opt['num_epochs']):\n",
        "            self.logger.info('------------ Start Epoch {} -----------'.format(epoch))\n",
        "            self.model.train()\n",
        "\n",
        "            # run batch\n",
        "            total_batch = len(self.train_loader)\n",
        "            for step, (inputs, labels, indexes) in enumerate(self.train_loader):\n",
        "                # naive training\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                if self.adv_train:\n",
        "                    final_inputs = self.attacker.get_adv_images(inputs, labels)\n",
        "                else:\n",
        "                    final_inputs = inputs\n",
        "\n",
        "                # instrumental sampling training by running all samples parallelly\n",
        "                iter_info_print = {}\n",
        "                all_ces = []\n",
        "                all_regs = []\n",
        "                preds, z_scores, features, logits = self.model(final_inputs, loop=self.num_loop)\n",
        "                for i, logit in enumerate(logits):\n",
        "                    if self.config['inst_sample']['ce_smooth']:\n",
        "                        ce_loss = self.criterion(logit, labels, confidence=float(z_scores[i]))\n",
        "                    else:\n",
        "                        ce_loss = self.criterion(logit, labels)\n",
        "                    iter_info_print['ce_loss_{}'.format(i)] = ce_loss.sum().item()\n",
        "                    all_ces.append(ce_loss)\n",
        "\n",
        "                for i in range(len(features)):\n",
        "                    if self.config['inst_sample']['reg_loss'] == 'L2':\n",
        "                        reg_loss = self.l2_loss(features[i] * self.get_mean_wo_i(z_scores, i), self.get_mean_wo_i(features, i) * z_scores[i])\n",
        "                        iter_info_print['ciiv_l2loss_{}'.format(i)] = reg_loss.sum().item()\n",
        "                    elif self.config['inst_sample']['reg_loss'] == 'L1':\n",
        "                        reg_loss = self.smooth_l1_loss(features[i] * self.get_mean_wo_i(z_scores, i), self.get_mean_wo_i(features, i) * z_scores[i])\n",
        "                        iter_info_print['ciiv_l1loss_{}'.format(i)] = reg_loss.sum().item()\n",
        "                    else:\n",
        "                        raise ValueError('Wrong Reg Loss Type')\n",
        "                    all_regs.append(reg_loss)\n",
        "\n",
        "                loss = self.w_ce * sum(all_ces) / len(all_ces) + self.w_reg * sum(all_regs) / len(all_regs)\n",
        "                iter_info_print['w_ce'] = self.w_ce\n",
        "                iter_info_print['w_reg'] = self.w_reg\n",
        "\n",
        "                # backward\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "                # calculate accuracy\n",
        "                accuracy = (preds.max(1)[1] == labels).sum().float() / preds.shape[0]\n",
        "\n",
        "                # log information\n",
        "                iter_info_print.update( {'Accuracy' : accuracy.item(), 'Loss' : loss.sum().item(), 'Poke LR' : float(self.optimizer.param_groups[0]['lr'])} )\n",
        "                self.logger.info_iter(epoch, step, total_batch, iter_info_print, self.config['logger_opt']['print_iter'])\n",
        "                if self.config['logger_opt']['print_grad'] and step % 1000 == 0:\n",
        "                    utils.print_grad(self.model.named_parameters())\n",
        "\n",
        "            # evaluation on validation set\n",
        "            self.optimizer.zero_grad()\n",
        "            val_acc = self.testing.run_val(epoch)\n",
        "\n",
        "            # update regression loss weight for BPFC or Instrumental Sampling\n",
        "            if (epoch in self.config['inst_sample']['milestones']):\n",
        "                self.logger.info('update regression weight from {} to {}'.format(self.w_reg, self.w_reg * self.mul_ru))\n",
        "                self.w_reg = self.w_reg * self.mul_ru\n",
        "\n",
        "            # checkpoint\n",
        "            self.checkpoint.save(self.model, epoch, self.logger, acc=val_acc)\n",
        "            # update scheduler\n",
        "            self.scheduler.step()\n",
        "        # save best model path\n",
        "        self.checkpoint.save_best_model(self.logger)\n",
        "\n"
      ],
      "metadata": {
        "id": "yoZie0TFRkV9"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def update_attacker_info(config, attack_config, dataset_name, attacker_type, attacker_set):\n",
        "    print('==================== Attacker {} ================='.format(attacker_type))\n",
        "    config['attacker_opt']['attack_type'] = attacker_type\n",
        "    config['attacker_opt']['attack_set'] = attacker_set\n",
        "    config['attacker_opt'].update(attack_config[dataset_name][attacker_type][attacker_set])\n",
        "\n",
        "def RepresentsInt(s):\n",
        "    try:\n",
        "        int(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def RepresentsFloat(s):\n",
        "    try:\n",
        "        float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "def int_to_others(val):\n",
        "    if val == 'true' or val == 'True':\n",
        "        return True\n",
        "    elif val == 'false' or val == 'False':\n",
        "        return False\n",
        "    elif RepresentsInt(val):\n",
        "        return int(val)\n",
        "    elif RepresentsFloat(val):\n",
        "        return float(val)\n",
        "    else:\n",
        "        return val\n",
        "\n",
        "def update_config_key(update_dict, key, new_val):\n",
        "    names = key.split('.')\n",
        "    while(len(names) > 1):\n",
        "        item_key = names.pop(0)\n",
        "        update_dict = update_dict[item_key]\n",
        "    old_val = update_dict[names[-1]]\n",
        "    update_dict[names[-1]] = int_to_others(new_val)\n",
        "    return old_val\n",
        "\n",
        "def update(config, args, logger):\n",
        "    if args.output_dir is not None:\n",
        "        config['output_dir'] = args.output_dir\n",
        "        logger.info('======= Update Config: output_dir is set to : ' + str(config['output_dir']))\n",
        "    if args.train_type is not None:\n",
        "        config['strategy']['train_type'] = args.train_type\n",
        "        logger.info('======= Update Config: training type is set to: '.format(args.train_type))\n",
        "    if args.test_type is not None:\n",
        "        config['strategy']['test_type'] = args.test_type\n",
        "        logger.info('======= Update Config: test type is set to: '.format(args.test_type))\n",
        "    if args.adv_train:\n",
        "        config['attacker_opt']['adv_train'] = args.adv_train\n",
        "    if args.adv_test:\n",
        "        config['attacker_opt']['adv_val'] = args.adv_test\n",
        "    if args.adv_type is not None:\n",
        "        config['attacker_opt']['attack_type'] = args.adv_type\n",
        "    if args.adv_setting is not None:\n",
        "        config['attacker_opt']['attack_set'] = args.adv_setting\n",
        "    if args.rand_aug:\n",
        "        config['dataset']['rand_aug'] = True\n",
        "        logger.info('===================> Using Random Augmentation')\n",
        "\n",
        "    if args.target_type:\n",
        "        config['targeted_attack'] = True\n",
        "        config['targeted_type'] = args.target_type\n",
        "    else:\n",
        "        config['targeted_attack'] = False\n",
        "\n",
        "    # update config from command\n",
        "    if len(args.opts) > 0 and (len(args.opts) % 2) == 0:\n",
        "        for i in range(len(args.opts)//2):\n",
        "            key = args.opts[2*i]\n",
        "            val = args.opts[2*i+1]\n",
        "            old_val = update_config_key(config, key, val)\n",
        "            logger.info('=====> {}: {} (old key) => {} (new key)'.format(key, old_val, val))\n",
        "    return config\n",
        "\n",
        "def source_import(file_path):\n",
        "    \"\"\"This function imports python module directly from source code using importlib\"\"\"\n",
        "    spec = importlib.util.spec_from_file_location('', file_path)\n",
        "    module = importlib.util.module_from_spec(spec)\n",
        "    spec.loader.exec_module(module)\n",
        "    return module\n",
        "\n",
        "def print_grad(named_parameters):\n",
        "    \"\"\" show grads \"\"\"\n",
        "    total_norm = 0\n",
        "    param_to_norm = {}\n",
        "    param_to_shape = {}\n",
        "    for n, p in named_parameters:\n",
        "        if p.grad is not None:\n",
        "            param_norm = p.grad.data.norm(2)\n",
        "            total_norm += param_norm ** 2\n",
        "            param_to_norm[n] = param_norm\n",
        "            param_to_shape[n] = p.size()\n",
        "    total_norm = total_norm ** (1. / 2)\n",
        "    print('---Total norm {:.3f} -----------------'.format(total_norm))\n",
        "    for name, norm in sorted(param_to_norm.items(), key=lambda x: -x[1]):\n",
        "            print(\"{:<50s}: {:.3f}, ({})\".format(name, norm, param_to_shape[name]))\n",
        "    print('-------------------------------', flush=True)\n",
        "    return total_norm\n",
        "\n",
        "def print_config(config, logger, head=''):\n",
        "    for key, val in config.items():\n",
        "        if isinstance(val, dict):\n",
        "            logger.info(head + str(key))\n",
        "            print_config(val, logger, head=head + '   ')\n",
        "        else:\n",
        "            logger.info(head + '{} : {}'.format(str(key), str(val)))\n",
        "\n",
        "class TriggerAction():\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.action = {}\n",
        "\n",
        "    def add_action(self, name, func):\n",
        "        assert str(name) not in self.action\n",
        "        self.action[str(name)] = func\n",
        "\n",
        "    def remove_action(self, name):\n",
        "        assert str(name) in self.action\n",
        "        del self.action[str(name)]\n",
        "        assert str(name) not in self.action\n",
        "\n",
        "    def run_all(self, logger=None):\n",
        "        for key, func in self.action.items():\n",
        "            if logger:\n",
        "                logger.info('trigger {}'.format(key))\n",
        "            func()"
      ],
      "metadata": {
        "id": "i3ugKD8QRmEz"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "class custom_logger():\n",
        "    def __init__(self, output_path, name='logger.txt'):\n",
        "        now = datetime.now()\n",
        "        logger_name = str(now.strftime(\"20%y_%h_%d_\")) + name\n",
        "        self.logger_path = os.path.join(output_path, logger_name)\n",
        "        # init logger file\n",
        "        f = open(self.logger_path, \"w+\")\n",
        "        f.write(self.get_local_time() + 'Start Logging \\n')\n",
        "        f.close()\n",
        "\n",
        "    def get_local_time(self):\n",
        "        now = datetime.now()\n",
        "        return str(now.strftime(\"%y_%h_%d %H:%M:%S : \"))\n",
        "\n",
        "    def info(self, log_str):\n",
        "        print(str(log_str))\n",
        "        with open(self.logger_path, \"a\") as f:\n",
        "            f.write(self.get_local_time() + str(log_str) + '\\n')\n",
        "\n",
        "    def raise_error(self, error):\n",
        "        prototype = '************* Error: {} *************'.format(str(error))\n",
        "        self.info(prototype)\n",
        "        raise ValueError(str(error))\n",
        "\n",
        "    def info_iter(self, epoch, batch, total_batch, info_dict, print_iter):\n",
        "        if batch % print_iter != 0:\n",
        "            pass\n",
        "        else:\n",
        "            acc_log = 'Epoch {:5d}, Batch {:6d}/{},'.format(epoch, batch, total_batch)\n",
        "            for key, val in info_dict.items():\n",
        "                acc_log += ' {}: {:9.3f},'.format(str(key), float(val))\n",
        "            self.info(acc_log)"
      ],
      "metadata": {
        "id": "uB9VBAEZ72E3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuring the model:"
      ],
      "metadata": {
        "id": "cMnlNxBJJETQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/out'\n",
        "logger_name = 'log_train'\n",
        "phase = 'train'\n",
        "cfg = '/content/CiiV-pytorch/config/_cifar10_ciiv.yaml'\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.mkdir(output_dir)\n",
        "if phase != 'train':\n",
        "    logger = custom_logger(output_dir, name='{}.txt'.format(logger_name))\n",
        "else:\n",
        "    logger = custom_logger(output_dir)\n",
        "logger.info('========================= Start Main =========================')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P7WbWUE65uYP",
        "outputId": "c9a29a4a-c6b9-4e18-b98d-4f675be42bc4"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "========================= Start Main =========================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# fix random seed\n",
        "seed = 100\n",
        "logger.info('=====> Using fixed random seed: ' + str(seed))\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WN1_g2XY5vIc",
        "outputId": "b319d140-a0db-4a01-b615-adbd10886ce5"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====> Using fixed random seed: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load config\n",
        "logger.info('=====> Load config from yaml: ' + str(cfg))\n",
        "with open(cfg) as f:\n",
        "    config = yaml.safe_load(f)\n",
        "\n",
        "# load attacker config\n",
        "logger.info('=====> Load attacker config from yaml: config/attacker_config.yaml')\n",
        "with open('/content/CiiV-pytorch/config/attacker_config.yaml') as f:\n",
        "    attack_config = yaml.safe_load(f)\n",
        "\n",
        "# change attack type\n",
        "dataset_name = config['dataset']['name']\n",
        "attacker_type = config['attacker_opt']['attack_type']\n",
        "attacker_set  = config['attacker_opt']['attack_set']\n",
        "update_attacker_info(config, attack_config, dataset_name, attacker_type, attacker_set)\n",
        "\n",
        "# save config file\n",
        "logger.info('=====> Save config as config.json')\n",
        "with open(os.path.join(output_dir, 'config.json'), 'w') as f:\n",
        "    json.dump(config, f)\n",
        "print_config(config, logger)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iguujW4q5yFl",
        "outputId": "f7ed448c-abe7-4ec5-d848-73cdb61beb47"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====> Load config from yaml: /content/CiiV-pytorch/config/_cifar10_ciiv.yaml\n",
            "=====> Load attacker config from yaml: config/attacker_config.yaml\n",
            "==================== Attacker FGSM =================\n",
            "=====> Save config as config.json\n",
            "output_dir : /content/out\n",
            "dataset\n",
            "   name : cifar10\n",
            "   data_path : /content/CiiV-pytorch/data/CIFAR10\n",
            "   img_size : 32\n",
            "   rand_aug : False\n",
            "networks\n",
            "   def_file : ./models/ResNet_CiiV.py\n",
            "   params\n",
            "      m_type : resnet18\n",
            "      num_classes : 10\n",
            "      num_sample : 3\n",
            "      aug_weight : 0.9\n",
            "      mask_center : [16, 5, 27]\n",
            "strategy\n",
            "   train_type : ciiv\n",
            "   test_type : ciiv\n",
            "attacker_opt\n",
            "   adv_val : False\n",
            "   adv_train : False\n",
            "   attack_type : FGSM\n",
            "   attack_set : setting1\n",
            "   eot_iter : 1\n",
            "   attack_eps : 8\n",
            "   attack_rand_ini : 1.0\n",
            "training_opt\n",
            "   num_epochs : 110\n",
            "   batch_size : 128\n",
            "   data_workers : 4\n",
            "   optimizer : SGD\n",
            "   optim_params\n",
            "      lr : 0.1\n",
            "      momentum : 0.9\n",
            "      weight_decay : 0.0005\n",
            "   scheduler : step\n",
            "   scheduler_params\n",
            "      endlr : 0.0\n",
            "      gamma : 0.1\n",
            "      milestones : [10, 100, 105]\n",
            "inst_sample\n",
            "   w_ce : 1\n",
            "   w_reg : 0.1\n",
            "   mul_ru : 10\n",
            "   milestones : [25, 50, 75]\n",
            "   num_loop : 9\n",
            "   reg_loss : L1\n",
            "   ce_smooth : False\n",
            "test_opt\n",
            "   save_data : False\n",
            "   file_name : adv_images.pth\n",
            "   save_length : 5\n",
            "logger_opt\n",
            "   print_grad : False\n",
            "   print_iter : 100\n",
            "checkpoint_opt\n",
            "   save_best : True\n",
            "   checkpoint_step : 10\n",
            "   checkpoint_name : test_model.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create model\n",
        "logger.info('=====> Model construction from: ' + str(config['networks']['def_file']))\n",
        "model_file = '/content/CiiV-pytorch/models/ResNet_CiiV.py' #config['networks']['def_file']\n",
        "model_args = config['networks']['params']\n",
        "model = source_import(model_file).create_model(**model_args) #create_model(**model_args)\n",
        "model.config = config\n",
        "model = nn.DataParallel(model).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iUW9cdrhQFtQ",
        "outputId": "9a6d7ecd-8c70-4392-a458-0cb2bbd4172d"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=====> Model construction from: ./models/ResNet_CiiV.py\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "ReLU\n",
            "Use activation of ReLU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the model:"
      ],
      "metadata": {
        "id": "0H3pXISAOKrk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training = train_ciiv(config, logger, model)\n",
        "training.run()"
      ],
      "metadata": {
        "id": "f1hwEUsGnD21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "933d53e1-ad80-4df5-9284-a2ca5895d153"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============= Training Strategy: CiiV Training ============\n",
            "=====> Get train dataloader\n",
            "Files already downloaded and verified\n",
            "=====> Save dataset info\n",
            "=====> Init Instrumental Sampling\n",
            "=====> Create optimizer\n",
            "=====> Create Scheduler\n",
            "=====> Init Instrumental Sampling\n",
            "Files already downloaded and verified\n",
            "=====> Save dataset info\n",
            "=====> Start Naive Training\n",
            "------------ Start Epoch 0 -----------\n",
            "Epoch     0, Batch      0/391, ce_loss_0:     2.347, ce_loss_1:     2.349, ce_loss_2:     2.355, ce_loss_3:     2.331, ce_loss_4:     2.349, ce_loss_5:     2.354, ce_loss_6:     2.346, ce_loss_7:     2.345, ce_loss_8:     2.344, ciiv_l1loss_0:     4.147, ciiv_l1loss_1:     4.178, ciiv_l1loss_2:     4.226, ciiv_l1loss_3:     4.149, ciiv_l1loss_4:     4.146, ciiv_l1loss_5:     4.148, ciiv_l1loss_6:     4.227, ciiv_l1loss_7:     4.234, ciiv_l1loss_8:     4.197, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.141, Loss:     2.765, Poke LR:     0.100,\n",
            "Epoch     0, Batch    100/391, ce_loss_0:     1.848, ce_loss_1:     1.830, ce_loss_2:     1.834, ce_loss_3:     1.854, ce_loss_4:     1.843, ce_loss_5:     1.844, ce_loss_6:     1.837, ce_loss_7:     1.842, ce_loss_8:     1.844, ciiv_l1loss_0:     0.064, ciiv_l1loss_1:     0.058, ciiv_l1loss_2:     0.082, ciiv_l1loss_3:     0.075, ciiv_l1loss_4:     0.067, ciiv_l1loss_5:     0.054, ciiv_l1loss_6:     0.068, ciiv_l1loss_7:     0.097, ciiv_l1loss_8:     0.081, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.289, Loss:     1.849, Poke LR:     0.100,\n",
            "Epoch     0, Batch    200/391, ce_loss_0:     1.777, ce_loss_1:     1.792, ce_loss_2:     1.779, ce_loss_3:     1.789, ce_loss_4:     1.821, ce_loss_5:     1.805, ce_loss_6:     1.790, ce_loss_7:     1.818, ce_loss_8:     1.804, ciiv_l1loss_0:     0.046, ciiv_l1loss_1:     0.061, ciiv_l1loss_2:     0.073, ciiv_l1loss_3:     0.074, ciiv_l1loss_4:     0.052, ciiv_l1loss_5:     0.072, ciiv_l1loss_6:     0.066, ciiv_l1loss_7:     0.067, ciiv_l1loss_8:     0.071, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.352, Loss:     1.804, Poke LR:     0.100,\n",
            "Epoch     0, Batch    300/391, ce_loss_0:     1.677, ce_loss_1:     1.695, ce_loss_2:     1.664, ce_loss_3:     1.692, ce_loss_4:     1.682, ce_loss_5:     1.677, ce_loss_6:     1.673, ce_loss_7:     1.660, ce_loss_8:     1.678, ciiv_l1loss_0:     0.040, ciiv_l1loss_1:     0.058, ciiv_l1loss_2:     0.051, ciiv_l1loss_3:     0.046, ciiv_l1loss_4:     0.049, ciiv_l1loss_5:     0.050, ciiv_l1loss_6:     0.048, ciiv_l1loss_7:     0.051, ciiv_l1loss_8:     0.058, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.359, Loss:     1.683, Poke LR:     0.100,\n",
            "------------- Start Validation at Epoch 0 -----------\n",
            "Epoch     0 Evaluation Complete ==> Total Accuracy :    0.4786, Number Samples :     10000\n",
            "Best model is updated at epoch 0 with accuracy     0.479 (Path: /content/out/epoch_0_test_model.pth)\n",
            "Model at epoch 0 is saved to /content/out/epoch_0_test_model.pth\n",
            "Best model is at epoch 0 with accuracy     0.479\n",
            "Best model is at epoch 0 with accuracy     0.479 (Path: /content/out/epoch_0_test_model.pth)\n",
            "------------ Start Epoch 1 -----------\n",
            "Epoch     1, Batch      0/391, ce_loss_0:     1.538, ce_loss_1:     1.516, ce_loss_2:     1.570, ce_loss_3:     1.555, ce_loss_4:     1.542, ce_loss_5:     1.547, ce_loss_6:     1.544, ce_loss_7:     1.533, ce_loss_8:     1.550, ciiv_l1loss_0:     0.054, ciiv_l1loss_1:     0.064, ciiv_l1loss_2:     0.077, ciiv_l1loss_3:     0.064, ciiv_l1loss_4:     0.060, ciiv_l1loss_5:     0.068, ciiv_l1loss_6:     0.066, ciiv_l1loss_7:     0.078, ciiv_l1loss_8:     0.073, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.422, Loss:     1.551, Poke LR:     0.100,\n",
            "Epoch     1, Batch    100/391, ce_loss_0:     1.300, ce_loss_1:     1.283, ce_loss_2:     1.288, ce_loss_3:     1.318, ce_loss_4:     1.303, ce_loss_5:     1.281, ce_loss_6:     1.323, ce_loss_7:     1.305, ce_loss_8:     1.325, ciiv_l1loss_0:     0.056, ciiv_l1loss_1:     0.072, ciiv_l1loss_2:     0.080, ciiv_l1loss_3:     0.053, ciiv_l1loss_4:     0.075, ciiv_l1loss_5:     0.063, ciiv_l1loss_6:     0.065, ciiv_l1loss_7:     0.067, ciiv_l1loss_8:     0.098, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.539, Loss:     1.310, Poke LR:     0.100,\n",
            "Epoch     1, Batch    200/391, ce_loss_0:     1.424, ce_loss_1:     1.469, ce_loss_2:     1.438, ce_loss_3:     1.467, ce_loss_4:     1.454, ce_loss_5:     1.452, ce_loss_6:     1.485, ce_loss_7:     1.447, ce_loss_8:     1.422, ciiv_l1loss_0:     0.096, ciiv_l1loss_1:     0.126, ciiv_l1loss_2:     0.105, ciiv_l1loss_3:     0.100, ciiv_l1loss_4:     0.115, ciiv_l1loss_5:     0.119, ciiv_l1loss_6:     0.126, ciiv_l1loss_7:     0.106, ciiv_l1loss_8:     0.114, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.500, Loss:     1.462, Poke LR:     0.100,\n",
            "Epoch     1, Batch    300/391, ce_loss_0:     1.185, ce_loss_1:     1.107, ce_loss_2:     1.159, ce_loss_3:     1.173, ce_loss_4:     1.212, ce_loss_5:     1.173, ce_loss_6:     1.162, ce_loss_7:     1.162, ce_loss_8:     1.150, ciiv_l1loss_0:     0.124, ciiv_l1loss_1:     0.138, ciiv_l1loss_2:     0.155, ciiv_l1loss_3:     0.139, ciiv_l1loss_4:     0.165, ciiv_l1loss_5:     0.151, ciiv_l1loss_6:     0.160, ciiv_l1loss_7:     0.136, ciiv_l1loss_8:     0.183, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.586, Loss:     1.180, Poke LR:     0.100,\n",
            "------------- Start Validation at Epoch 1 -----------\n",
            "Epoch     1 Evaluation Complete ==> Total Accuracy :    0.5867, Number Samples :     10000\n",
            "Best model is updated at epoch 1 with accuracy     0.587 (Path: /content/out/epoch_1_test_model.pth)\n",
            "Model at epoch 1 is saved to /content/out/epoch_1_test_model.pth\n",
            "Best model is at epoch 1 with accuracy     0.587\n",
            "Best model is at epoch 1 with accuracy     0.587 (Path: /content/out/epoch_1_test_model.pth)\n",
            "------------ Start Epoch 2 -----------\n",
            "Epoch     2, Batch      0/391, ce_loss_0:     1.243, ce_loss_1:     1.217, ce_loss_2:     1.231, ce_loss_3:     1.229, ce_loss_4:     1.265, ce_loss_5:     1.224, ce_loss_6:     1.206, ce_loss_7:     1.249, ce_loss_8:     1.291, ciiv_l1loss_0:     0.110, ciiv_l1loss_1:     0.117, ciiv_l1loss_2:     0.119, ciiv_l1loss_3:     0.112, ciiv_l1loss_4:     0.112, ciiv_l1loss_5:     0.118, ciiv_l1loss_6:     0.126, ciiv_l1loss_7:     0.118, ciiv_l1loss_8:     0.116, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.547, Loss:     1.251, Poke LR:     0.100,\n",
            "Epoch     2, Batch    100/391, ce_loss_0:     1.080, ce_loss_1:     1.087, ce_loss_2:     1.100, ce_loss_3:     1.113, ce_loss_4:     1.099, ce_loss_5:     1.123, ce_loss_6:     1.101, ce_loss_7:     1.158, ce_loss_8:     1.113, ciiv_l1loss_0:     0.107, ciiv_l1loss_1:     0.143, ciiv_l1loss_2:     0.142, ciiv_l1loss_3:     0.123, ciiv_l1loss_4:     0.151, ciiv_l1loss_5:     0.139, ciiv_l1loss_6:     0.149, ciiv_l1loss_7:     0.156, ciiv_l1loss_8:     0.145, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.656, Loss:     1.122, Poke LR:     0.100,\n",
            "Epoch     2, Batch    200/391, ce_loss_0:     1.095, ce_loss_1:     1.116, ce_loss_2:     1.050, ce_loss_3:     1.090, ce_loss_4:     1.090, ce_loss_5:     1.063, ce_loss_6:     1.064, ce_loss_7:     1.105, ce_loss_8:     1.090, ciiv_l1loss_0:     0.109, ciiv_l1loss_1:     0.130, ciiv_l1loss_2:     0.141, ciiv_l1loss_3:     0.108, ciiv_l1loss_4:     0.127, ciiv_l1loss_5:     0.135, ciiv_l1loss_6:     0.122, ciiv_l1loss_7:     0.102, ciiv_l1loss_8:     0.125, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.609, Loss:     1.097, Poke LR:     0.100,\n",
            "Epoch     2, Batch    300/391, ce_loss_0:     1.153, ce_loss_1:     1.205, ce_loss_2:     1.202, ce_loss_3:     1.162, ce_loss_4:     1.211, ce_loss_5:     1.163, ce_loss_6:     1.176, ce_loss_7:     1.165, ce_loss_8:     1.183, ciiv_l1loss_0:     0.119, ciiv_l1loss_1:     0.132, ciiv_l1loss_2:     0.146, ciiv_l1loss_3:     0.133, ciiv_l1loss_4:     0.139, ciiv_l1loss_5:     0.132, ciiv_l1loss_6:     0.160, ciiv_l1loss_7:     0.126, ciiv_l1loss_8:     0.136, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.570, Loss:     1.194, Poke LR:     0.100,\n",
            "------------- Start Validation at Epoch 2 -----------\n",
            "Epoch     2 Evaluation Complete ==> Total Accuracy :    0.6469, Number Samples :     10000\n",
            "Best model is updated at epoch 2 with accuracy     0.647 (Path: /content/out/epoch_2_test_model.pth)\n",
            "Model at epoch 2 is saved to /content/out/epoch_2_test_model.pth\n",
            "Best model is at epoch 2 with accuracy     0.647\n",
            "Best model is at epoch 2 with accuracy     0.647 (Path: /content/out/epoch_2_test_model.pth)\n",
            "------------ Start Epoch 3 -----------\n",
            "Epoch     3, Batch      0/391, ce_loss_0:     1.049, ce_loss_1:     1.088, ce_loss_2:     1.085, ce_loss_3:     1.075, ce_loss_4:     1.002, ce_loss_5:     1.036, ce_loss_6:     1.061, ce_loss_7:     1.029, ce_loss_8:     1.077, ciiv_l1loss_0:     0.119, ciiv_l1loss_1:     0.146, ciiv_l1loss_2:     0.147, ciiv_l1loss_3:     0.135, ciiv_l1loss_4:     0.146, ciiv_l1loss_5:     0.146, ciiv_l1loss_6:     0.137, ciiv_l1loss_7:     0.150, ciiv_l1loss_8:     0.143, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.664, Loss:     1.070, Poke LR:     0.100,\n",
            "Epoch     3, Batch    100/391, ce_loss_0:     0.974, ce_loss_1:     0.979, ce_loss_2:     0.949, ce_loss_3:     0.951, ce_loss_4:     0.929, ce_loss_5:     0.889, ce_loss_6:     0.941, ce_loss_7:     0.929, ce_loss_8:     0.940, ciiv_l1loss_0:     0.150, ciiv_l1loss_1:     0.174, ciiv_l1loss_2:     0.188, ciiv_l1loss_3:     0.173, ciiv_l1loss_4:     0.154, ciiv_l1loss_5:     0.161, ciiv_l1loss_6:     0.151, ciiv_l1loss_7:     0.163, ciiv_l1loss_8:     0.155, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.648, Loss:     0.959, Poke LR:     0.100,\n",
            "Epoch     3, Batch    200/391, ce_loss_0:     1.091, ce_loss_1:     1.077, ce_loss_2:     1.098, ce_loss_3:     1.073, ce_loss_4:     1.072, ce_loss_5:     1.065, ce_loss_6:     1.056, ce_loss_7:     1.075, ce_loss_8:     1.108, ciiv_l1loss_0:     0.119, ciiv_l1loss_1:     0.139, ciiv_l1loss_2:     0.160, ciiv_l1loss_3:     0.128, ciiv_l1loss_4:     0.152, ciiv_l1loss_5:     0.141, ciiv_l1loss_6:     0.134, ciiv_l1loss_7:     0.150, ciiv_l1loss_8:     0.166, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.617, Loss:     1.094, Poke LR:     0.100,\n",
            "Epoch     3, Batch    300/391, ce_loss_0:     0.910, ce_loss_1:     0.932, ce_loss_2:     0.922, ce_loss_3:     0.909, ce_loss_4:     0.949, ce_loss_5:     0.905, ce_loss_6:     0.930, ce_loss_7:     0.950, ce_loss_8:     0.965, ciiv_l1loss_0:     0.124, ciiv_l1loss_1:     0.139, ciiv_l1loss_2:     0.156, ciiv_l1loss_3:     0.130, ciiv_l1loss_4:     0.140, ciiv_l1loss_5:     0.144, ciiv_l1loss_6:     0.133, ciiv_l1loss_7:     0.153, ciiv_l1loss_8:     0.152, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.688, Loss:     0.944, Poke LR:     0.100,\n",
            "------------- Start Validation at Epoch 3 -----------\n",
            "Epoch     3 Evaluation Complete ==> Total Accuracy :    0.7023, Number Samples :     10000\n",
            "Best model is updated at epoch 3 with accuracy     0.702 (Path: /content/out/epoch_3_test_model.pth)\n",
            "Model at epoch 3 is saved to /content/out/epoch_3_test_model.pth\n",
            "Best model is at epoch 3 with accuracy     0.702\n",
            "Best model is at epoch 3 with accuracy     0.702 (Path: /content/out/epoch_3_test_model.pth)\n",
            "------------ Start Epoch 4 -----------\n",
            "Epoch     4, Batch      0/391, ce_loss_0:     1.161, ce_loss_1:     1.117, ce_loss_2:     1.172, ce_loss_3:     1.144, ce_loss_4:     1.149, ce_loss_5:     1.125, ce_loss_6:     1.108, ce_loss_7:     1.144, ce_loss_8:     1.182, ciiv_l1loss_0:     0.133, ciiv_l1loss_1:     0.138, ciiv_l1loss_2:     0.147, ciiv_l1loss_3:     0.136, ciiv_l1loss_4:     0.168, ciiv_l1loss_5:     0.136, ciiv_l1loss_6:     0.132, ciiv_l1loss_7:     0.166, ciiv_l1loss_8:     0.187, w_ce:     1.000, w_reg:     0.100, Accuracy:     0.609, Loss:     1.159, Poke LR:     0.100,\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-55fc9eb32e65>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ciiv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-69-ced432a02f05>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m                 \u001b[0;31m# log information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0miter_info_print\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Loss'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Poke LR'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miter_info_print\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logger_opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_iter'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'logger_opt'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'print_grad'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Strart Validation:"
      ],
      "metadata": {
        "id": "l1tPTdyGONib"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# normal test\n",
        "load_dir = '/content/out/epoch_3_test_model.pth'\n",
        "checkpoint = Checkpoint(config)\n",
        "checkpoint.load(model, load_dir, logger)\n",
        "\n",
        "# start testing\n",
        "test_func = get_test_func(config)\n",
        "\n",
        "# val:\n",
        "phase = 'val'\n",
        "testing = test_func(config, logger, model, val=True)\n",
        "testing.run_val(epoch=-1)\n",
        "\n",
        "phase = 'test'\n",
        "testing = test_func(config, logger, model, val=False)\n",
        "testing.run_val(epoch=-1)\n",
        "logger.info('========================= Complete =========================')"
      ],
      "metadata": {
        "id": "VJRd0DTUsnUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba5be521-9747-4bfb-a364-350ee1717520"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading checkpoint pretrained with epoch 3.\n",
            "Load                                module.conv1.weight from checkpoint.\n",
            "Load                         module.layer1.0.bn1.weight from checkpoint.\n",
            "Load                           module.layer1.0.bn1.bias from checkpoint.\n",
            "Load                   module.layer1.0.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer1.0.bn1.running_var from checkpoint.\n",
            "Load            module.layer1.0.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer1.0.conv1.weight from checkpoint.\n",
            "Load                         module.layer1.0.bn2.weight from checkpoint.\n",
            "Load                           module.layer1.0.bn2.bias from checkpoint.\n",
            "Load                   module.layer1.0.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer1.0.bn2.running_var from checkpoint.\n",
            "Load            module.layer1.0.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer1.0.conv2.weight from checkpoint.\n",
            "Load                         module.layer1.1.bn1.weight from checkpoint.\n",
            "Load                           module.layer1.1.bn1.bias from checkpoint.\n",
            "Load                   module.layer1.1.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer1.1.bn1.running_var from checkpoint.\n",
            "Load            module.layer1.1.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer1.1.conv1.weight from checkpoint.\n",
            "Load                         module.layer1.1.bn2.weight from checkpoint.\n",
            "Load                           module.layer1.1.bn2.bias from checkpoint.\n",
            "Load                   module.layer1.1.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer1.1.bn2.running_var from checkpoint.\n",
            "Load            module.layer1.1.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer1.1.conv2.weight from checkpoint.\n",
            "Load                         module.layer2.0.bn1.weight from checkpoint.\n",
            "Load                           module.layer2.0.bn1.bias from checkpoint.\n",
            "Load                   module.layer2.0.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer2.0.bn1.running_var from checkpoint.\n",
            "Load            module.layer2.0.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer2.0.conv1.weight from checkpoint.\n",
            "Load                         module.layer2.0.bn2.weight from checkpoint.\n",
            "Load                           module.layer2.0.bn2.bias from checkpoint.\n",
            "Load                   module.layer2.0.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer2.0.bn2.running_var from checkpoint.\n",
            "Load            module.layer2.0.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer2.0.conv2.weight from checkpoint.\n",
            "Load                  module.layer2.0.shortcut.0.weight from checkpoint.\n",
            "Load                         module.layer2.1.bn1.weight from checkpoint.\n",
            "Load                           module.layer2.1.bn1.bias from checkpoint.\n",
            "Load                   module.layer2.1.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer2.1.bn1.running_var from checkpoint.\n",
            "Load            module.layer2.1.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer2.1.conv1.weight from checkpoint.\n",
            "Load                         module.layer2.1.bn2.weight from checkpoint.\n",
            "Load                           module.layer2.1.bn2.bias from checkpoint.\n",
            "Load                   module.layer2.1.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer2.1.bn2.running_var from checkpoint.\n",
            "Load            module.layer2.1.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer2.1.conv2.weight from checkpoint.\n",
            "Load                         module.layer3.0.bn1.weight from checkpoint.\n",
            "Load                           module.layer3.0.bn1.bias from checkpoint.\n",
            "Load                   module.layer3.0.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer3.0.bn1.running_var from checkpoint.\n",
            "Load            module.layer3.0.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer3.0.conv1.weight from checkpoint.\n",
            "Load                         module.layer3.0.bn2.weight from checkpoint.\n",
            "Load                           module.layer3.0.bn2.bias from checkpoint.\n",
            "Load                   module.layer3.0.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer3.0.bn2.running_var from checkpoint.\n",
            "Load            module.layer3.0.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer3.0.conv2.weight from checkpoint.\n",
            "Load                  module.layer3.0.shortcut.0.weight from checkpoint.\n",
            "Load                         module.layer3.1.bn1.weight from checkpoint.\n",
            "Load                           module.layer3.1.bn1.bias from checkpoint.\n",
            "Load                   module.layer3.1.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer3.1.bn1.running_var from checkpoint.\n",
            "Load            module.layer3.1.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer3.1.conv1.weight from checkpoint.\n",
            "Load                         module.layer3.1.bn2.weight from checkpoint.\n",
            "Load                           module.layer3.1.bn2.bias from checkpoint.\n",
            "Load                   module.layer3.1.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer3.1.bn2.running_var from checkpoint.\n",
            "Load            module.layer3.1.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer3.1.conv2.weight from checkpoint.\n",
            "Load                         module.layer4.0.bn1.weight from checkpoint.\n",
            "Load                           module.layer4.0.bn1.bias from checkpoint.\n",
            "Load                   module.layer4.0.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer4.0.bn1.running_var from checkpoint.\n",
            "Load            module.layer4.0.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer4.0.conv1.weight from checkpoint.\n",
            "Load                         module.layer4.0.bn2.weight from checkpoint.\n",
            "Load                           module.layer4.0.bn2.bias from checkpoint.\n",
            "Load                   module.layer4.0.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer4.0.bn2.running_var from checkpoint.\n",
            "Load            module.layer4.0.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer4.0.conv2.weight from checkpoint.\n",
            "Load                  module.layer4.0.shortcut.0.weight from checkpoint.\n",
            "Load                         module.layer4.1.bn1.weight from checkpoint.\n",
            "Load                           module.layer4.1.bn1.bias from checkpoint.\n",
            "Load                   module.layer4.1.bn1.running_mean from checkpoint.\n",
            "Load                    module.layer4.1.bn1.running_var from checkpoint.\n",
            "Load            module.layer4.1.bn1.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer4.1.conv1.weight from checkpoint.\n",
            "Load                         module.layer4.1.bn2.weight from checkpoint.\n",
            "Load                           module.layer4.1.bn2.bias from checkpoint.\n",
            "Load                   module.layer4.1.bn2.running_mean from checkpoint.\n",
            "Load                    module.layer4.1.bn2.running_var from checkpoint.\n",
            "Load            module.layer4.1.bn2.num_batches_tracked from checkpoint.\n",
            "Load                       module.layer4.1.conv2.weight from checkpoint.\n",
            "Load                                   module.bn.weight from checkpoint.\n",
            "Load                                     module.bn.bias from checkpoint.\n",
            "Load                             module.bn.running_mean from checkpoint.\n",
            "Load                              module.bn.running_var from checkpoint.\n",
            "Load                      module.bn.num_batches_tracked from checkpoint.\n",
            "Load                               module.linear.weight from checkpoint.\n",
            "Load                                 module.linear.bias from checkpoint.\n",
            "=====> Init Instrumental Sampling\n",
            "Files already downloaded and verified\n",
            "=====> Save dataset info\n",
            "------------- Start Validation at Epoch -1 -----------\n",
            "Epoch    -1 Evaluation Complete ==> Total Accuracy :    0.7008, Number Samples :     10000\n",
            "=====> Init Instrumental Sampling\n",
            "Files already downloaded and verified\n",
            "=====> Save dataset info\n",
            "------------- Start Validation at Epoch -1 -----------\n",
            "Epoch    -1 Evaluation Complete ==> Total Accuracy :    0.7004, Number Samples :     10000\n",
            "========================= Complete =========================\n"
          ]
        }
      ]
    }
  ]
}
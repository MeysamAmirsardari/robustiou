{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Setup and Install Dependencies"
      ],
      "metadata": {
        "id": "8gGFTG8oVKqf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vctpmncjSxP-"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchaudio torchaudio-transform\n",
        "\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Loading and Preprocessing"
      ],
      "metadata": {
        "id": "IfQdjyIOVQ8H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchaudio\n",
        "!wget https://download.pytorch.org/tutorial_data.tar.gz\n",
        "!tar -xf tutorial_data.tar.gz\n",
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "import torchaudio\n",
        "from torchaudio.transforms import Resample\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "DATA_DIR = Path(\"./data/\")\n",
        "LABELS_FILE = DATA_DIR / \"labels.txt\"\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "def load_data_and_labels(data_dir, labels_file):\n",
        "    data = []\n",
        "    labels = []\n",
        "\n",
        "    with open(labels_file, \"r\") as file:\n",
        "        label_dict = {line.strip(): i for i, line in enumerate(file)}\n",
        "\n",
        "    for folder in data_dir.iterdir():\n",
        "        if folder.is_dir():\n",
        "            label = label_dict.get(folder.name, -1)\n",
        "            if label != -1:\n",
        "                for file in folder.glob(\"*.wav\"):\n",
        "                    waveform, _ = torchaudio.load(file)\n",
        "                    # Resample if\n",
        "                    if waveform.shape[1] != SAMPLE_RATE:\n",
        "                        resample = Resample(orig_freq=waveform.shape[1], new_freq=SAMPLE_RATE)\n",
        "                        waveform = resample(waveform)\n",
        "                    data.append(waveform)\n",
        "                    labels.append(label)\n",
        "\n",
        "    return data, labels\n",
        "\n",
        "class SpeechDataset(Dataset):\n",
        "    def __init__(self, data, labels):\n",
        "        self.data = data\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index], self.labels[index]\n",
        "\n",
        "data, labels = load_data_and_labels(DATA_DIR, LABELS_FILE)\n",
        "\n",
        "speech_dataset = SpeechDataset(data, labels)\n",
        "\n",
        "sample_data, sample_label = speech_dataset[0]\n",
        "print(f\"Sample Data Shape: {sample_data.shape}\")\n",
        "print(f\"Sample Label: {sample_label}\")\n"
      ],
      "metadata": {
        "id": "-fo-UM5pS0vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Definition and Training using VGGish"
      ],
      "metadata": {
        "id": "xHQrQ19OVVgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "class VGGish(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super(VGGish, self).__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
        "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
        "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
        "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(512 * (SAMPLE_RATE // 16) // 16, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, 4096),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(4096, num_classes),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "num_classes = len(set(labels))\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(data, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "train_loader = DataLoader(SpeechDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(SpeechDataset(val_data, val_labels), batch_size=batch_size)\n",
        "\n",
        "model = VGGish(num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs.unsqueeze(1))  # Add channel dim\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    average_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {average_loss:.4f}\")\n",
        "\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs.unsqueeze(1))  # Add channel dim\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "PFY75plPTFrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation and Prediction"
      ],
      "metadata": {
        "id": "fo6tDQ40VaWM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/download.tensorflow.org/data/speech_commands_test_set_v0.02.tar.gz\n",
        "!tar -xf speech_commands_test_set_v0.02.tar.gz\n",
        "\n",
        "TEST_DATA_DIR = Path(\"./speech_commands_test_set_v0.02\")\n",
        "TEST_LABELS_FILE = TEST_DATA_DIR / \"testing_list.txt\"\n",
        "\n",
        "test_data, test_labels = load_data_and_labels(TEST_DATA_DIR, TEST_LABELS_FILE)\n",
        "\n",
        "test_loader = DataLoader(SpeechDataset(test_data, test_labels), batch_size=batch_size)\n",
        "\n",
        "# Evaluate on test\n",
        "model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs.unsqueeze(1))  # Add channel dimension\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "test_accuracy = correct / total\n",
        "print(f\"Test Accuracy: {test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the trained model\n",
        "torch.save(model.state_dict(), \"speech_model.pth\")\n",
        "print(\"Model saved successfully.\")\n",
        "\n",
        "\n",
        "sample_index = 0\n",
        "sample_input, sample_label = test_data[sample_index], test_labels[sample_index]\n",
        "sample_input = torch.tensor(sample_input, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "sample_output = model(sample_input.unsqueeze(1))\n",
        "_, predicted_label = torch.max(sample_output, 1)\n",
        "\n",
        "print(f\"\\nExample Prediction:\")\n",
        "print(f\"True Label: {sample_label}, Predicted Label: {predicted_label.item()}\")\n"
      ],
      "metadata": {
        "id": "j6PV7qFtTXsR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Inference on New Data"
      ],
      "metadata": {
        "id": "8_vjr539VfDY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(audio_path):\n",
        "    waveform, _ = torchaudio.load(audio_path, normalize=True)\n",
        "\n",
        "    #TODO\n",
        "\n",
        "    return waveform\n",
        "\n",
        "# Load the saved model\n",
        "loaded_model = VGGish(num_classes)\n",
        "loaded_model.load_state_dict(torch.load(\"speech_model.pth\"))\n",
        "loaded_model.to(device)\n",
        "loaded_model.eval()\n",
        "\n",
        "\n",
        "new_audio_path = \"/path/to/your/new/audio/sample.wav\"\n",
        "preprocessed_audio = preprocess_audio(new_audio_path)\n",
        "input_tensor = torch.tensor(preprocessed_audio, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "output = loaded_model(input_tensor.unsqueeze(1))  # Add channel dimension\n",
        "_, predicted_label = torch.max(output, 1)\n",
        "\n",
        "print(\"\\nInference on New Data:\")\n",
        "print(f\"Predicted Label: {predicted_label.item()}\")\n"
      ],
      "metadata": {
        "id": "sOAQZOZ5Terh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Visualization of Model Predictions"
      ],
      "metadata": {
        "id": "wuEfogFKVkxS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def visualize_predictions(model, data_loader, num_samples=5):\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(num_samples, 2, figsize=(12, 3 * num_samples))\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(data_loader):\n",
        "            if i >= num_samples:\n",
        "                break\n",
        "\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs.unsqueeze(1))  # Add channel dimension\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "\n",
        "            axes[i, 0].plot(inputs[0].cpu().numpy(), color='blue')\n",
        "            axes[i, 0].set_title(f\"True Label: {labels.item()}\")\n",
        "\n",
        "            # predicted probabilities\n",
        "            probabilities = torch.nn.functional.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "            class_labels = list(range(num_classes))\n",
        "            axes[i, 1].bar(class_labels, probabilities, color='green')\n",
        "            axes[i, 1].set_xticks(class_labels)\n",
        "            axes[i, 1].set_title(f\"Predicted Label: {predicted.item()}\")\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions(loaded_model, test_loader, num_samples=5)\n"
      ],
      "metadata": {
        "id": "YiPfI0DGTnxZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}